# -*- coding: utf-8 -*-
"""Linear_Regression_Model.ipynb by Teghpreet Singh

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LPZegeNEjOSC73MbnMtYAf7yTEky_qtO
"""

# Linear regression - attempts to model the relationship between two variables by fitting a linear equation to observed data. 
# A linear regression line has an equation of the form Y = a + bX

!pip3 install torch

import torch
from torch.nn import Linear
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

# Create a Dataset of 100 rows and 1 column
X = torch.randn(100, 1) * 10
Y = X + 3*torch.randn(100,1)      # Y is a function of X, shift it up and down to increase noise
plt.plot( X, Y, 'o')
plt.ylabel('Y')
plt.xlabel('X')

# we start with a random value for weight and bias
# then we train our model through a gradient descent algorithm
# to obtain optimal parameters to fit our data.

# Linear Regression is a sub class of nn.Module inheriting all nueral network modules from this parent class
# w/ inheritance dont forget to call super().__init__():  # useful in multiple inheritance 
# of parent classes

class LinearRegression(nn.Module):
  def __init__(self, input_size, output_size ):
    super().__init__()
    self.linear = nn.Linear(input_size, output_size)
  def forward(self, x):
    prod = self.linear(x)
    return prod

torch.manual_seed(1)

# Create a Model Instance of the Linear Regression Class
model = LinearRegression(1,1)

[w, b] = model.parameters() # 2D Tensor with 1 Row and 1 Column
def get_params():
  return (w[0][0].item(), b[0].item())

def plot_fit(title):
  plt.title = title
  w1, b1 = get_params() # get the weight and bias values from the model
  x1 = np.array([-30,30])
  print(x1) # array of 2 elements -30 and 30
  y1 = w1 * x1 + b1
  plt.plot(x1, y1, 'r')
  plt.scatter(X, Y)
  plt.show()

plot_fit('Initial Model')

# Implement Gradient Descent

# Loss Function
criterion = nn.MSELoss()

#will use SGD to update the weights and bias
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)

# 1 epoch is a single pass through the entire Dataset, we calculate the error function and backpropogate 
# the gradient of the error function to update the weights
epochs = 100
losses = []

# Most Models we TRAIN will follow this similar process
#
# 1. Make predictions using our Model
# 2. Compare the predictions made by the model to the actual outputs
# 3. Based on that
# 4. Determine the Loss Mean Squared
# 5. Then use the SGD to update the weights of our model in the direction of least error
# 6. Thereby minimizing the error function of our model as we attempt to minimize the loss iteratively
#

# we want to minimize the error in each epoch pass
for i in range(epochs):
  y_pred = model.forward(X) # 1. Make predictions using our Model
  loss = criterion(y_pred, Y) # 2. Compare the predictions made by the model to the actual outputs, to determine MSE, Mean squared error
  print("epoch: ", i, "loss: ", loss.item())
  losses.append(loss)
  optimizer.zero_grad() # set the gradient to zero, since gradients accumulate following loss.backward() call from previous epoch
  loss.backward()     # take the gradient of the loss function, we use loss.backward() to computer gradient
  optimizer.step()    # we update our model parameters by optimizer.step(), can be called once gradients are computed

plt.plot(range(epochs), losses)
plt.ylabel('Loss')
plt.xlabel('Epoch')

plot_fit('Trained Model')
