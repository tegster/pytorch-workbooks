# -*- coding: utf-8 -*-
"""Linear_Regression_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LPZegeNEjOSC73MbnMtYAf7yTEky_qtO
"""

# Linear regression - attempts to model the relationship between two variables by fitting a linear equation to observed data. 
# A linear regression line has an equation of the form Y = a + bX, also known as Y = mX + B

!pip3 install torch

import torch
from torch.nn import Linear
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

# 
# Create a Dataset of 100 rows and 1 column for our Linear Regression to Use
#

X = torch.randn(100, 1) * 10
Y = X + 3*torch.randn(100,1)      # Y is a function of X, shift it up and down to increase noise
plt.plot( X, Y, 'o')
plt.ylabel('Y')
plt.xlabel('X')

#
# we start with random values for weight and bias paramerters.
# then we train our model through a gradient descent algorithm
# to obtain optimal parameters for the weight and to fit our data.
#

# 
# Linear Regression class below is a sub class we created
# from nn.Module - inheriting all nueral network modules from this parent class
# nn.Module - Base class for all neural network modules - https://pytorch.org/docs/stable/generated/torch.nn.Module.html
# w/ inheritance we call super().__init__():  
# super() gives you access to methods in a superclass from the subclass that inherits from it.
#

class LinearRegression(nn.Module):
  def __init__(self, input_size, output_size ):
    super().__init__()
    self.linear = nn.Linear(input_size, output_size)

#
# Forward function returns the Y Value for given X, using our nn.linear computation provided by Pytorch
# Applies a linear transformation to the incoming data using the Weight and Bias defined by the model
# 
  def forward(self, x):
    Y_prod = self.linear(x)
    return Y_prod

torch.manual_seed(1)

# Create a Model Instance of the Linear Regression Class
model = LinearRegression(1,1)

[w, b] = model.parameters() # Parameters are returned in a 2D Tensor with 1 Row and 1 Column
def get_params():
  return (w[0][0].item(), b[0].item())

#
# The plot_fit function will print out the graph of all the X and Y datapoints 
# It will grab the weight and bias parameters from the model and multiple times -30 and 30 X points
# which will give us the Line of best fit 

def plot_fit(title):
  plt.title = title
  w1, b1 = get_params() # get the weight and bias values from the model
  x1 = np.array([-30,30])
  print(x1) # array of 2 elements -30 and 30
  y1 = w1 * x1 + b1
  plt.plot(x1, y1, 'r')
  plt.scatter(X, Y)
  plt.show()

plot_fit('Initial Model')

# 
# Implement Gradient Descent
#

# Loss Function
criterion = nn.MSELoss()

#will use SGD to update the weights and bias
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)

# 1 epoch is a single pass through the entire Dataset, we calculate the error function and backpropogate 
# the gradient of the error function to update the weights
epochs = 100
losses = []

# Most Models we TRAIN will follow this similar process
#
# 1. Make predictions using our Model
# 2. Compare the predictions made by the model to the actual outputs
# 3. Based on that
# 4. Determine the Loss Mean Squared
# 5. Then use the SGD to update the weights of our model in the direction of least error
# 6. Thereby minimizing the error function of our model as we attempt to minimize the loss iteratively
#

# we want to minimize the error in each epoch pass
for i in range(epochs):
  y_pred = model.forward(X) # 1. Make predictions using our Model
  loss = criterion(y_pred, Y) # 2. Compare the predictions made by the model to the actual outputs, to determine MSE, Mean squared error
  print("epoch: ", i, "loss: ", loss.item())
  losses.append(loss)
  optimizer.zero_grad() # set the gradient to zero, since gradients accumulate following loss.backward() call from previous epoch
  loss.backward()     # take the gradient of the loss function, we use loss.backward() to computer gradient
  optimizer.step()    # we update our model parameters by optimizer.step(), can be called once gradients are computed

plt.plot(range(epochs), losses)
plt.ylabel('Loss')
plt.xlabel('Epoch')

plot_fit('Trained Model')
